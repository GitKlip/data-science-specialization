---
title: "capstone"
output: 
  html_document: 
    number_sections: yes
    toc: yes
    theme: cerulean
    highlight: tango
date: '`r format(Sys.time(), ''%d %B, %Y'')`'
setspace: single


---

*****************************************************
# The Project Context
Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. When someone types: "I went to the" the keyboard presents three options for what the next word might be. For example, the three words might be gym, store, restaurant. In this capstone you will work on understanding and building predictive text models like those used by SwiftKey.

This course will start with the basics, analyzing a large corpus of text documents to discover the structure in the data and how words are put together. It will cover cleaning and analyzing text data, then building and sampling from a predictive text model. Finally, you will use the knowledge you gained in data products to build a predictive text product you can show off to your family, friends, and potential employers.

You will use all of the skills you have learned during the Data Science Specialization in this course, but you'll notice that we are tackling a brand new application: analysis of text data and natural language processing. This choice is on purpose. As a practicing data scientist you will be frequently confronted with new data types and problems. A big part of the fun and challenge of being a data scientist is figuring out how to work with these new data types to build data products people love. 

The capstone will be evaluated based on the following assessments:  

	- An introductory quiz to test whether you have downloaded and can manipulate the data.
	- An intermediate R markdown report that describes in plain language, plots, and code your exploratory analysis of the course data set.
	- Two natural language processing quizzes, where you apply your predictive model to real data to check how it is working.
	- A Shiny app that takes as input a phrase (multiple words), one clicks submit, and it predicts the next word.
	- A 5 slide deck created with R presentations pitching your algorithm and app to your boss or investor.
	- During the capstone you can get support from your fellow students, from us, and from the engineers at SwiftKey. But we really want you to show your independence, creativity, and initiative. We have been incredibly impressed by your performance in the classes up until now and know you can do great things.

We have compiled some basic natural language processing resources below. You are welcome to use these resources or any others you can find while performing this analysis. One thing to keep in mind is that we do not expect you to become a world's expert in natural language processing. The point of this capstone is for you to show you can explore a new data type, quickly get up to speed on a new application, and implement a useful model in a reasonable period of time. We think NLP is very cool and depending on your future goals may be worth studying more in-depth, but you can complete this project by using your general knowledge of data science and basic knowledge of NLP.

	- Text mining infrastucture in R [http://www.jstatsoft.org/v25/i05/]
	- CRAN Task View: Natural Language Processing [http://cran.r-project.org/web/views/NaturalLanguageProcessing.html]
	- Stanford Natural Language Processing MOOC (starts September 2016) [https://www.coursera.org/learn/nlp]
	
# Tasks Overview
this course will be separated into 8 different tasks that cover the range of activities encountered by a practicing data scientist. They mirror many of the skills you have developed in the data science specialization. The tasks are:

- Understanding the problem
- Data acquisition and cleaning
- Exploratory analysis
- Statistical modeling
- Predictive modeling
- Creative exploration
- Creating a data product
- Creating a short slide deck pitching your product
	

*****************************************************
#Task 0 - Understanding the problem

The first step in analyzing any new data set is figuring out: (a) what data you have and (b) what are the standard tools and models used for that type of data. Make sure you have downloaded the data from Coursera before heading for the exercises. This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora (www.corpora.heliohost.org). See the readme file at http://www.corpora.heliohost.org/aboutcorpus.html for details on the corpora available. The files have been language filtered but may still contain some foreign text.

In this capstone we will be applying data science in the area of natural language processing. As a first step toward working on this project, you should familiarize yourself with Natural Language Processing, Text Mining, and the associated tools in R. Here are some resources that may be helpful to you.

* Natural language processing Wikipedia page [https://en.wikipedia.org/wiki/Natural_language_processing]
* Text mining infrastucture in R [http://www.jstatsoft.org/v25/i05/]
* CRAN Task View: Natural Language Processing [http://cran.r-project.org/web/views/NaturalLanguageProcessing.html]
* Stanford Coursera course on NLP (not in R) [https://www.coursera.org/course/nlp]

##Dataset

This is the training data to get you started that will be the basis for most of the capstone. You must download the data from the Coursera site and not from external websites to start.
Capstone Dataset [https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip]
Your original exploration of the data and modeling steps will be performed on this data set. Later in the capstone, if you find additional data sets that may be useful for building your model you may use them.

##Tasks to accomplish

1. Obtaining the data - Can you download the data and load/manipulate it in R?
1. Familiarizing yourself with NLP and text mining - Learn about the basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.

Questions to consider

1. What do the data look like?
    each file comes from a different source
    each row is a sample/partial text.  each row is it's own observation.
    file size (MB):
        blogs - 
        news - 
        twitter - 
    num rows:
        blogs - 899049 rows. fread friendly.
        news - 1010242 rows. fread friendly.
        twitter - 2360148 rows.  fread fail, used readlines to deal with four rows had embedded nulls that needed to be skipped.
    longest line:
        blogs - 
        news - 
        twitter - 
    loveCount/hateCount: 
        twitter - 
    top 10 words
        blogs - 
        news - 
        twitter - 
    'biostats' context
        twitter - 
    count tweets with "A computer once beat me at chess, but it was no match for me at kickboxing":
        twitter - 
        
    
1. Where do the data come from?
1. Can you think of any other data sources that might help you in this project?
    profanity list
    
1. What are the common steps in natural language processing?
1. What are some common issues in the analysis of text data?
    profanity - permutations, slang, creative people
1. What is the relationship between NLP and the concepts you have learned in the Specialization?
	

##Common Pre-Processing Steps

- Using R's tm() library
- Data import as plain text. 
	Reuters <- Corpus(); tmMap(); tmFilter();
- stemming.  erase word suffixes. reduces complexity. used with matrix or bag-of-words
	stemDoc()
- whitespace removal and case conversion
	stripWhitespace();tmTolower()
- stopword removal. (extreemly common words that add no value like 'and,for,is,it')
	removeWords(); tmMap(); stopwords(language='en')
- identify synonyms and replace all related words with a single word.  
	library("wordnet");synonyms();replaceWords();tmMap()
- Tag words with their Part Of Speach (POS). 
	tagPOS()

## Common Analysis Approaches

- COunt-based evaluation (frequencies of words, frequencies of phrases)
	remove stopwords
	create TermDocMatrix()
	determine count cutoff to designate important words;
	find most important terms in each doc
- Association-based evaluation (find associations for a given word)
	findAssocs(crudeTDM, "oil", .85)
- Clustering (group documents according to their texts in unknown/undefined groups)
	Hiearchal / Tree - wsHClust()
	k-means clustering - kmeans()
- Simple text classification (group documents into specified groups)
	k-nearest neighbor classification - train then predict
	support vector machine classification - train then predict
- Text clustering with string kernels (work with text directly. efficent work on lots of dimensions)	
	

*****************************************************
# 	Task 1 - Getting and Cleaning Data
Large databases comprising of text in a target language are commonly used when generating language models for various purposes. In this exercise, you will use the **English database** but may consider three other databases in German, Russian and Finnish.

The goal of this task is to get familiar with the databases and do the necessary cleaning. After this exercise, you should understand what real data looks like and how much effort you need to put into cleaning the data. When you commence on developing a new language, the first thing is to understand the language and its peculiarities with respect to your target. You can learn to read, speak and write the language. Alternatively, you can study data and learn from existing information about the language through literature and the internet. At the very least, you need to understand how the language is written: writing script, existing input methods, some phonetic knowledge, etc.

Note that the data contain words of offensive and profane meaning. They are left there intentionally to highlight the fact that the developer has to work on them.

**Tasks to accomplish**

1. ***Tokenization*** - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
1. ***Profanity filtering*** - removing profanity and other words you do not want to predict.


```{r}

source('getAndLoadData.R')
str(news10)



#Filter profanity and useless words


##Common Pre-Processing Steps

# - Using R's tm() library
# - Data import as plain text. 
# 	Reuters <- Corpus(); tmMap(); tmFilter();
# - stemming.  erase word suffixes. reduces complexity. used with matrix or bag-of-words
# 	stemDoc()
# - whitespace removal and case conversion
# 	stripWhitespace();tmTolower()
# - stopword removal. (extreemly common words that add no value like 'and,for,is,it')
# 	removeWords(); tmMap(); stopwords(language='en')
# - identify synonyms and replace all related words with a single word.  
# 	library("wordnet");synonyms();replaceWords();tmMap()
# - Tag words with their Part Of Speach (POS). 
# 	tagPOS()
# 
# install.packages('tm',dependencies=TRUE)
# library(tm)
# ?Corpus()
# ?reader
# ?tm_map()
# tm_filter()
# 
# file
```
	
